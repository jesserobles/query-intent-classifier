{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f14f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "from dataprocessor.conll import DatasetCombiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440f7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, task=\"ner\", label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84b7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(dataset):\n",
    "    label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_list))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    return label_list, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5541f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\jesse\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39eb2b5a487489d920593e08d021800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at C:\\Users\\jesse\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee\\cache-5f2aea3fa5a2ad5d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f2a4cb6b894a178b82e2c306888052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\jesse\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee\\cache-b5637782ee4e68d3.arrow\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\jesse/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14042\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 09:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.114660</td>\n",
       "      <td>0.864528</td>\n",
       "      <td>0.873249</td>\n",
       "      <td>0.868867</td>\n",
       "      <td>0.971066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.125360</td>\n",
       "      <td>0.875015</td>\n",
       "      <td>0.889865</td>\n",
       "      <td>0.882377</td>\n",
       "      <td>0.972723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.130604</td>\n",
       "      <td>0.893117</td>\n",
       "      <td>0.888559</td>\n",
       "      <td>0.890832</td>\n",
       "      <td>0.974829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>0.889505</td>\n",
       "      <td>0.895205</td>\n",
       "      <td>0.892346</td>\n",
       "      <td>0.974812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.154291</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.891645</td>\n",
       "      <td>0.890958</td>\n",
       "      <td>0.975036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.158897</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.898528</td>\n",
       "      <td>0.897251</td>\n",
       "      <td>0.976020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>0.888221</td>\n",
       "      <td>0.894968</td>\n",
       "      <td>0.891582</td>\n",
       "      <td>0.974967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.173031</td>\n",
       "      <td>0.891148</td>\n",
       "      <td>0.899715</td>\n",
       "      <td>0.895411</td>\n",
       "      <td>0.975606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.174154</td>\n",
       "      <td>0.889670</td>\n",
       "      <td>0.901495</td>\n",
       "      <td>0.895544</td>\n",
       "      <td>0.975589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.175273</td>\n",
       "      <td>0.893539</td>\n",
       "      <td>0.899478</td>\n",
       "      <td>0.896499</td>\n",
       "      <td>0.975796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3500\n",
      "Configuration saved in ./results\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-4000\n",
      "Configuration saved in ./results\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-4500\n",
      "Configuration saved in ./results\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-5000\n",
      "Configuration saved in ./results\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-5500\n",
      "Configuration saved in ./results\\checkpoint-5500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-6000\n",
      "Configuration saved in ./results\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-6000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-6500\n",
      "Configuration saved in ./results\\checkpoint-6500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-6500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-7000\n",
      "Configuration saved in ./results\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-7000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-7500\n",
      "Configuration saved in ./results\\checkpoint-7500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-7500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./results\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-7500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-8000\n",
      "Configuration saved in ./results\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-8500\n",
      "Configuration saved in ./results\\checkpoint-8500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8780, training_loss=0.03044775820131454, metrics={'train_runtime': 553.6488, 'train_samples_per_second': 253.627, 'train_steps_per_second': 15.858, 'total_flos': 1701127258035660.0, 'train_loss': 0.03044775820131454, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "label_list, trainer = build_trainer(dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d613cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id, chunk_tags, pos_tags. If ner_tags, tokens, id, chunk_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17527250945568085,\n",
       " 'eval_precision': 0.8935392596085829,\n",
       " 'eval_recall': 0.8994778067885117,\n",
       " 'eval_f1': 0.8964986988407855,\n",
       " 'eval_accuracy': 0.9757958704509357,\n",
       " 'eval_runtime': 4.5329,\n",
       " 'eval_samples_per_second': 761.989,\n",
       " 'eval_steps_per_second': 47.652,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f43e10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58841800f47a4ef8b4bf0a139b7302a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3969b0e73694223b56eeb737bfbf018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4478\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/2800 02:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.218185</td>\n",
       "      <td>0.265490</td>\n",
       "      <td>0.252288</td>\n",
       "      <td>0.258720</td>\n",
       "      <td>0.701515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>2.444638</td>\n",
       "      <td>0.266964</td>\n",
       "      <td>0.255644</td>\n",
       "      <td>0.261181</td>\n",
       "      <td>0.701515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>2.615385</td>\n",
       "      <td>0.270626</td>\n",
       "      <td>0.257169</td>\n",
       "      <td>0.263726</td>\n",
       "      <td>0.702356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>2.720783</td>\n",
       "      <td>0.270418</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.263306</td>\n",
       "      <td>0.702146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>2.802723</td>\n",
       "      <td>0.270679</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.701935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>2.866261</td>\n",
       "      <td>0.271001</td>\n",
       "      <td>0.256864</td>\n",
       "      <td>0.263743</td>\n",
       "      <td>0.701935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>2.908258</td>\n",
       "      <td>0.270592</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.263389</td>\n",
       "      <td>0.701830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>2.938429</td>\n",
       "      <td>0.270331</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.263265</td>\n",
       "      <td>0.701725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>2.959070</td>\n",
       "      <td>0.269923</td>\n",
       "      <td>0.256254</td>\n",
       "      <td>0.262911</td>\n",
       "      <td>0.701515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>2.966643</td>\n",
       "      <td>0.270331</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.263265</td>\n",
       "      <td>0.701830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jesse\\Anaconda3\\envs\\hug\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2800, training_loss=0.13044223700250898, metrics={'train_runtime': 141.4856, 'train_samples_per_second': 316.499, 'train_steps_per_second': 19.79, 'total_flos': 271311782348448.0, 'train_loss': 0.13044223700250898, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_combiner = DatasetCombiner(os.path.join(\"datasets\", \"ATIS\"))\n",
    "dataset = dataset_combiner.dataset\n",
    "label_list, trainer = build_trainer(dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaf4b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.9666426181793213,\n",
       " 'eval_precision': 0.27033108325297334,\n",
       " 'eval_recall': 0.2565588773642465,\n",
       " 'eval_f1': 0.26326498669588355,\n",
       " 'eval_accuracy': 0.7018300378628524,\n",
       " 'eval_runtime': 0.8378,\n",
       " 'eval_samples_per_second': 1065.94,\n",
       " 'eval_steps_per_second': 66.845,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae7479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
